{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\final-year-project\\real-final-code\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import Seq2Seq\n",
    "from utils import Example, convert_examples_to_features\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We are defining all the needed functions here. \n",
    "def inference(data, model, tokenizer):\n",
    "    # Calculate bleu\n",
    "    eval_sampler = SequentialSampler(data)\n",
    "    eval_dataloader = DataLoader(data, sampler=eval_sampler, batch_size=len(data))\n",
    "\n",
    "    model.eval()\n",
    "    p = []\n",
    "    for batch in eval_dataloader:\n",
    "        batch = tuple(t.to('cpu') for t in batch)\n",
    "        source_ids, source_mask = batch\n",
    "        with torch.no_grad():\n",
    "            preds = model(source_ids=source_ids, source_mask=source_mask)\n",
    "            for pred in preds:\n",
    "                t = pred[0].cpu().numpy()\n",
    "                t = list(t)\n",
    "                if 0 in t:\n",
    "                    t = t[: t.index(0)]\n",
    "                text = tokenizer.decode(t, clean_up_tokenization_spaces=False)\n",
    "                p.append(text)\n",
    "    return (p, source_ids.shape[-1])\n",
    "\n",
    "\n",
    "def get_features(examples, tokenizer):\n",
    "    features = convert_examples_to_features(\n",
    "        examples, tokenizer, stage=\"test\"\n",
    "    )\n",
    "    all_source_ids = torch.tensor(\n",
    "        [f.source_ids[: 256] for f in features], dtype=torch.long\n",
    "    )\n",
    "    all_source_mask = torch.tensor(\n",
    "        [f.source_mask[: 256] for f in features], dtype=torch.long\n",
    "    )\n",
    "    return TensorDataset(all_source_ids, all_source_mask)\n",
    "\n",
    "\n",
    "def build_model(model_class, config, tokenizer):\n",
    "    encoder = model_class(config=config)\n",
    "    decoder_layer = nn.TransformerDecoderLayer(\n",
    "        d_model=config.hidden_size, nhead=config.num_attention_heads\n",
    "    )\n",
    "    decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "    model = Seq2Seq(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        config=config,\n",
    "        beam_size=10,\n",
    "        max_length=128,\n",
    "        sos_id=tokenizer.cls_token_id,\n",
    "        eos_id=tokenizer.sep_token_id,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(\n",
    "        torch.load(\n",
    "            \"pytorch_model.bin\",\n",
    "            map_location=torch.device(\"cpu\"),\n",
    "        ),\n",
    "        strict=False,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig.from_pretrained(\"microsoft/codebert-base\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\n",
    "    \"microsoft/codebert-base\", do_lower_case=False\n",
    ")\n",
    "\n",
    "model = build_model(\n",
    "    model_class=RobertaModel, config=config, tokenizer=tokenizer\n",
    ").to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message:  ['Checks that all numbers in a list are equal .']\n"
     ]
    }
   ],
   "source": [
    "for i in os.listdir('files-for-summarization'):\n",
    "    with open('files-for-summarization/user_code.py', 'r') as f:\n",
    "        body = f.read()\n",
    "    example = [Example(source=body, target=None)]\n",
    "    message, length = inference(get_features(example, tokenizer), model, tokenizer)\n",
    "    print('message: ', message)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import importlib\n",
    "\n",
    "\n",
    "def function_dissimator(module_name, path_to_module):\n",
    "    spec = importlib.util.spec_from_file_location(module_name, path_to_module)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    funcs = []\n",
    "    for name, value in vars(module).items():\n",
    "        if name.startswith(\"_\") or not callable(value):\n",
    "            continue\n",
    "        doc = inspect.getdoc(value)\n",
    "        code = inspect.getsource(value).split(\":\", maxsplit=1)[1]\n",
    "        funcs.append({\"name\": name, \"docstring\": doc, \"body\": code})\n",
    "\n",
    "    return funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open('summary.txt', 'w') as summary:\n",
    "    for i in os.listdir('files-from-zip'):\n",
    "        if os.path.splitext(i)[1] == '.py':\n",
    "            summary.write(f'\\nFile name: {i}\\n')\n",
    "            for func in function_dissimator(os.path.splitext(i)[0], f'files-from-zip/{i}'):\n",
    "                summary.write(f'Function name: {func[\"name\"]}\\n')\n",
    "                body = func[\"body\"]\n",
    "                example = [Example(source=body, target=None)]\n",
    "                message, length = inference(get_features(example, tokenizer), model, tokenizer)\n",
    "                summary.write(f'Summary: {message}\\n\\n')\n",
    "    \n",
    "summary.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
